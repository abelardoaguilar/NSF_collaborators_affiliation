{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_name = \"A_LUQUE\"\n",
    "output_path = \"../results\"\n",
    "generate_individual_files = False\n",
    "include_timestamp = True\n",
    "global_json_file = os.path.join(output_path, f'{author_name}_all_articles_extensive_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orcid_articles(orcid_id):\n",
    "    # Replace 'YOUR_ACCESS_TOKEN' with an actual ORCID API access token if needed.\n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        #'Authorization': 'Bearer YOUR_ACCESS_TOKEN',\n",
    "    }\n",
    "    \n",
    "    # Construct the URL to access the ORCID record\n",
    "    url = f'https://pub.orcid.org/v3.0/{orcid_id}/works'\n",
    "    \n",
    "    # Make the request to the ORCID API\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        dois = []\n",
    "        \n",
    "        # Loop through the returned works and extract the DOIs\n",
    "        for work in data.get('group', []):\n",
    "            for work_summary in work.get('work-summary', []):\n",
    "                doi = work_summary.get('external-ids', {}).get('external-id', [])\n",
    "                for id in doi:\n",
    "                    if id.get('external-id-type') == 'doi':\n",
    "                        dois.append(id.get('external-id-value'))\n",
    "        \n",
    "        return dois\n",
    "    else:\n",
    "        print(f'Failed to retrieve data for ORCID ID {orcid_id}. Status code: {response.status_code}')\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_crossref_articles(orcid_id):\n",
    "    \"\"\"\n",
    "    Retrieves a list of DOIs for publications associated with a given ORCID ID from CrossRef.\n",
    "\n",
    "    Parameters:\n",
    "    orcid_id (str): The ORCID ID of the author.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of DOIs for the author's publications.\n",
    "    \"\"\"\n",
    "    # Base URL for CrossRef API\n",
    "    crossref_api_url = \"https://api.crossref.org/works\"\n",
    "    # Parameters for the API request, filtering by ORCID ID\n",
    "    params = {\n",
    "        'filter': f'orcid:{orcid_id}',\n",
    "        'rows': 1000  # Adjust the number of results as needed\n",
    "    }\n",
    "    \n",
    "    # Perform the API request\n",
    "    response = requests.get(crossref_api_url, params=params)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Extract DOIs from the items in the response\n",
    "        dois = [item['DOI'] for item in data['message']['items']]\n",
    "        return dois\n",
    "    else:\n",
    "        print(f\"Error fetching data: HTTP {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_combined_dois(orcid_id):\n",
    "    orcid_dois = get_orcid_articles(orcid_id)\n",
    "    crossref_dois = get_crossref_articles(orcid_id)\n",
    "    combined_dois = orcid_dois + crossref_dois\n",
    "    unique_dois = list(set(combined_dois))\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"DOIs for ORCID ID:{orcid_id}\\n\")\n",
    "    print(f\"Number of DOIs from ORCID: {len(orcid_dois)}\")\n",
    "    print(f\"Number of DOIs from CrossRef: {len(crossref_dois)}\")\n",
    "    print(f\"Total number of unique DOIs: {len(unique_dois)}\")\n",
    "    \n",
    "    return unique_dois\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOIs for ORCID ID:0000-0002-5817-4914\n",
      "\n",
      "Number of DOIs from ORCID: 47\n",
      "Number of DOIs from CrossRef: 17\n",
      "Total number of unique DOIs: 39\n",
      "['10.1038/s41467-019-12367-3', '10.1101/2023.04.20.537752', '10.1016/j.bpj.2016.04.024', '10.1128/mSystems.00353-20', '10.1080/10511970.2021.1881847', '10.1101/495481', '10.1111/1462-2920.15640', '10.1093/nar/gku491', '10.1063/1.4712304', '10.1088/1478-3975/9/3/036003', '10.1038/nmicrobiol.2017.64', '10.1038/nature17193', '10.1128/mBio.02207-17', '10.1101/2023.03.05.531146', '10.1093/bioinformatics/btad761', '10.1186/s12915-023-01571-9', '10.3390/microorganisms8121944', '10.1128/msystems.00353-20', '10.1099/mgen.0.001100', '10.1073/pnas.0915122107', '10.1016/j.bpj.2010.02.051', '10.1186/s12864-020-6523-2', '10.1007/978-94-007-6552-8_19', '10.1101/2023.12.27.573307', '10.1101/2023.02.27.529640', '10.1101/2020.04.22.056689', '10.20944/preprints202011.0024.v1', '10.3390/v14050973', '10.1101/327031', '10.1016/j.sbi.2015.04.002', '10.1038/s41598-019-52794-2', '10.1101/2023.11.03.565530', '10.3389/fphy.2021.594306', '10.1016/j.csbj.2021.12.032', '10.1128/mBio.01874-17', '10.1103/PhysRevLett.108.020604', '10.1016/j.bpj.2011.01.008', '10.7554/elife.49114', '10.1039/c3nr05763a']\n"
     ]
    }
   ],
   "source": [
    "# Example usage get_combined_dois\n",
    "orcid_id = \"0000-0002-5817-4914\" # Replace with the actual ORCID ID\n",
    "query_dois = get_combined_dois(orcid_id)\n",
    "print(query_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_article_data(doi):\n",
    "    \"\"\"\n",
    "    Fetches article data from CrossRef using DOI.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for DOI: {doi}\")\n",
    "        return None\n",
    "\n",
    "def extract_author_data(article_data):\n",
    "    \"\"\"\n",
    "    Extracts author data from CrossRef article data.\n",
    "    \"\"\"\n",
    "    authors = article_data['message']['author']\n",
    "    author_data = []\n",
    "    for author in authors:\n",
    "        author_info = {\n",
    "            'given_name': author.get('given', ''),\n",
    "            'family_name': author.get('family', ''),\n",
    "            'affiliation': []\n",
    "        }\n",
    "        if 'affiliation' in author:\n",
    "            if isinstance(author['affiliation'], list):\n",
    "                for affiliation in author['affiliation']:\n",
    "                    if isinstance(affiliation, str):\n",
    "                        author_info['affiliation'].append(affiliation)\n",
    "                    elif isinstance(affiliation, dict) and 'name' in affiliation:\n",
    "                        author_info['affiliation'].append(affiliation['name'])\n",
    "            elif isinstance(author['affiliation'], str):\n",
    "                author_info['affiliation'].append(author['affiliation'])\n",
    "        author_data.append(author_info)\n",
    "    return author_data\n",
    "\n",
    "def save_article_data_to_json(article_data, author_data, doi):\n",
    "    \"\"\"\n",
    "    Saves article data and author data to a JSON file.\n",
    "    \"\"\"\n",
    "    article_message = article_data.get('message', {})\n",
    "    \n",
    "    title = article_message.get('title', [''])[0]\n",
    "    \n",
    "    container_title = article_message.get('container-title', [])\n",
    "    journal = container_title[0] if container_title else ''\n",
    "    \n",
    "    publication_date = ''\n",
    "    if 'published-online' in article_message:\n",
    "        date_parts = article_message['published-online'].get('date-parts', [[]])\n",
    "        if date_parts:\n",
    "            publication_date = date_parts[0][0]\n",
    "\n",
    "    filename = f'article_data_{doi.replace(\"/\", \"_\")}.json'\n",
    "    \n",
    "    data_to_save = {\n",
    "        'article': {\n",
    "            'title': title,\n",
    "            'journal': journal,\n",
    "            'doi': doi,\n",
    "            'publication_date': publication_date\n",
    "        },\n",
    "        'authors': author_data\n",
    "    }\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data_to_save, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_global_json_file(dois, generate_individual_files):\n",
    "    all_articles_data = []\n",
    "\n",
    "    for doi in dois:\n",
    "        article_data = fetch_article_data(doi)\n",
    "        if article_data:\n",
    "            author_data = extract_author_data(article_data)\n",
    "            print(f\"Authors data for DOI {doi}:\")\n",
    "            for author in author_data:\n",
    "                print(f\"Name: {author['given_name']} {author['family_name']}\")\n",
    "                print(f\"Affiliation(s): {', '.join(author['affiliation']) if author['affiliation'] else 'Not available'}\")\n",
    "                print()\n",
    "            print()\n",
    "            if generate_individual_files:\n",
    "                save_article_data_to_json(article_data, author_data, doi) # This line execution depends on the value of the variable generate_individual_files\n",
    "            if 'message' in article_data and 'title' in article_data['message'] and 'container-title' in article_data['message'] and 'published-online' in article_data['message']:\n",
    "                all_articles_data.append({\n",
    "                    'doi': doi,\n",
    "                    'article_data': {\n",
    "                        'title': article_data['message']['title'][0],\n",
    "                        'journal': article_data['message']['container-title'][0],\n",
    "                        'doi': doi,\n",
    "                        'publication_date': article_data['message']['published-online']['date-parts'][0][0]\n",
    "                    },\n",
    "                    'author_data': author_data\n",
    "                })\n",
    "\n",
    "    with open(global_json_file, 'w') as f:\n",
    "        json.dump(all_articles_data, f, indent=4)\n",
    "\n",
    "\n",
    "def generate_global_df(global_json_file):\n",
    "    # Read the contents of the global_json_file\n",
    "    with open(global_json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Create an empty list to store the rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate over the data and create a row for each article+author combination\n",
    "    for article_data in data:\n",
    "        doi = article_data['doi']\n",
    "        title = article_data['article_data']['title']\n",
    "        journal = article_data['article_data']['journal']\n",
    "        publication_date = article_data['article_data']['publication_date']\n",
    "        authors = article_data['author_data']\n",
    "\n",
    "        for author in authors:\n",
    "            given_name = author['given_name']\n",
    "            family_name = author['family_name']\n",
    "            affiliation = ', '.join(author['affiliation']) if author['affiliation'] else 'Not available'\n",
    "\n",
    "            row = {\n",
    "                'DOI': doi,\n",
    "                'Title': title,\n",
    "                'Journal': journal,\n",
    "                'Publication Date': publication_date,\n",
    "                'Given Name': given_name,\n",
    "                'Family Name': family_name,\n",
    "                'Affiliation': affiliation\n",
    "            }\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    # Create the dataframe\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               DOI  \\\n",
      "0       10.1038/s41467-019-12367-3   \n",
      "1       10.1038/s41467-019-12367-3   \n",
      "2    10.1080/10511970.2021.1881847   \n",
      "3    10.1080/10511970.2021.1881847   \n",
      "4    10.1080/10511970.2021.1881847   \n",
      "..                             ...   \n",
      "186             10.1039/c3nr05763a   \n",
      "187             10.1039/c3nr05763a   \n",
      "188             10.1039/c3nr05763a   \n",
      "189             10.1039/c3nr05763a   \n",
      "190             10.1039/c3nr05763a   \n",
      "\n",
      "                                                 Title                Journal  \\\n",
      "0    Structural puzzles in virology solved with an ...  Nature Communications   \n",
      "1    Structural puzzles in virology solved with an ...  Nature Communications   \n",
      "2    Aligning Calculus with Life Sciences Disciplin...                 PRIMUS   \n",
      "3    Aligning Calculus with Life Sciences Disciplin...                 PRIMUS   \n",
      "4    Aligning Calculus with Life Sciences Disciplin...                 PRIMUS   \n",
      "..                                                 ...                    ...   \n",
      "186  The interplay between mechanics and stability ...              Nanoscale   \n",
      "187  The interplay between mechanics and stability ...              Nanoscale   \n",
      "188  The interplay between mechanics and stability ...              Nanoscale   \n",
      "189  The interplay between mechanics and stability ...              Nanoscale   \n",
      "190  The interplay between mechanics and stability ...              Nanoscale   \n",
      "\n",
      "     Publication Date Given Name Family Name  \\\n",
      "0                2019     Reidun     Twarock   \n",
      "1                2019     Antoni       Luque   \n",
      "2                2021     Antoni       Luque   \n",
      "3                2021      James    Mullinix   \n",
      "4                2021       Matt    Anderson   \n",
      "..                ...        ...         ...   \n",
      "186              2014    José R.      Castón   \n",
      "187              2014     Antoni       Luque   \n",
      "188              2014    José L.  Carrascosa   \n",
      "189              2014      David     Reguera   \n",
      "190              2014   Pedro J.    de Pablo   \n",
      "\n",
      "                                           Affiliation  \n",
      "0                                        Not available  \n",
      "1                                        Not available  \n",
      "2                                        Not available  \n",
      "3                                        Not available  \n",
      "4                                        Not available  \n",
      "..                                                 ...  \n",
      "186  Dept. de Estructura de Macromoléculas, Centro ...  \n",
      "187  Department of Chemistry, New York University, ...  \n",
      "188  Dept. de Estructura de Macromoléculas, Centro ...  \n",
      "189  Departament de Física Fonamental, Facultat de ...  \n",
      "190  Departamento de Física de la Materia Condensad...  \n",
      "\n",
      "[191 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "global_df = generate_global_df(global_json_file)\n",
    "print(global_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "create_authors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
